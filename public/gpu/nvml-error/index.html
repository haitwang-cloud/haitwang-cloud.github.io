<!doctype html>




<script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1313460867822205"
     crossorigin="anonymous"></script>





































<html
  class="not-ready lg:text-base"
  style="--bg: #f8f5d7"
  lang="zh-CN"
>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>如何解决 `Failed to initialize NVML: Unknown Error` 问题 - Tim Wang的技术博客</title>

  
  <meta name="theme-color" />

  
  
  
  
  <meta name="description" content="本文是NOTICE: Containers losing access to GPUs with error: &ldquo;Failed to initialize NVML: Unknown Error&rdquo;的中文翻译版本，内容有删减，亲测该方法有效。
如何解决 Failed to initialize NVML: Unknown Error 问题 概述 在某些特定的情况下，我们发现k8s容器可能会突然从最初连接到的GPU上分离。我们已经确定了这个问题的根本原因，并确定了可能发生这种情况的受影响环境。在本文档的末尾提供了受影响环境的解决方法，直到发布适当的修复为止。
问题总结 我们发现当使用container来管理GPU工作负载时，用户container可能会突然失去对GPU的访问权限。这种情况发生在使用systemd来管理容器的cgroups时，当触发重新加载任何包含对NVIDIA GPU的引用的Unit文件时（例如，通过执行systemctl daemon-reload）。
当你的container失去对GPU的访问权限时，你可能会看到类似于以下错误消息：
Failed to initialize NVML: Unknown Error 一旦发生上述 ⬆️，就需要手动删除受影响的container，然后重新启动它们。
当container重新启动（手动或自动，取决于是否使用容器编排平台），它将重新获得对GPU的访问权限。
此问题的根源在于，最近版本的runc要求在/dev/char下面为注入到容器中的任何设备节点提供符号链接。不幸的是，NVIDIA设备并没有这些符号链接，NVIDIA GPU驱动也没有（当前）提供自动创建这些链接的方法
受影响的环境 如果你使用runc并在高级容器运行时（CRI）启用systemd cgroup管理的环境，那么你可能会受到这个问题的影响
如果你没有使用systemd来管理cgroup，那么它就不会受到这个问题的影响。 下面是可能会受影响的的环境的详尽列表:
使用containerd/runc的Docker环境 特定条件 启用了systemd的cgroup驱动程序（例如，在/etc/docker/daemon.json中设置了参数&quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]） 使用了更新的Docker版本，其中systemd cgroup管理被默认设置的（即，在Ubuntu 22.04上）。 Note如果你要检查Docker是否使用systemd cgroup管理，运行以下命令（下面的输出表示启用了systemd cgroup驱动程序）
$ docker info ... Cgroup Driver: systemd Cgroup Version: 1 使用containerd/runc的Kubernetes环境 特定条件 在containerd配置文件（通常位于：/etc/containerd/config.toml）中设置SystemdCgroup = true，如下所示： [plugins." />
  <meta name="author" content="Tim Wang" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="http://localhost:1313/main.min.css" />

  
  
  
  
  
  <link rel="preload" as="image" href="http://localhost:1313/theme.png" />

  
  
  
  
  <link rel="preload" as="image" href="https://gravatar.com/haitaoking1993" />
  
  

  
  
  <link rel="preload" as="image" href="http://localhost:1313/github.svg" />
  
  <link rel="preload" as="image" href="http://localhost:1313/linkedin.svg" />
  
  <link rel="preload" as="image" href="http://localhost:1313/rss.svg" />
  
  

  
  
  <script
    defer
    src="http://localhost:1313/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  
  

  
  <link rel="icon" href="http://localhost:1313/favicon.ico" />
  <link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.127.0">

  
  
  
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[4.5rem] max-w-3xl px-8 lg:justify-center">
  <div class="relative z-50 mr-auto flex items-center">
    <a
      class="-translate-x-[1px] -translate-y-[1px] text-2xl font-semibold"
      href="http://localhost:1313/"
      >Tim Wang的技术博客</a
    >
    <div
      class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 -mr-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
    role="button"
    aria-label="Menu"
  ></div>

  

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#f8f5d7'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    
    <nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-6">
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/istio/"
        >Istio</a
      >
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/k8s/"
        >K8s</a
      >
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/gpu/"
        >GPU</a
      >
      
    </nav>
    

    
    <nav
      class="mt-12 flex justify-center space-x-10 dark:invert lg:ml-12 lg:mt-0 lg:items-center lg:space-x-6"
    >
      
      <a
        class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./github.svg)"
        href="https://github.com/haitwang-cloud"
        target="_blank"
        rel="me"
      >
        github
      </a>
      
      <a
        class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./linkedin.svg)"
        href="https://linkedin.com/in/tim-wang-505213166"
        target="_blank"
        rel="me"
      >
        linkedin
      </a>
      
      <a
        class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./rss.svg)"
        href="http://localhost:1313/index.xml"
        target="_blank"
        rel="alternate"
      >
        rss
      </a>
      
    </nav>
    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-3xl px-8 pb-16 pt-12 dark:prose-invert"
    >
      

<article>
  <header class="mb-16">
    <h1 class="!my-0 pb-2.5">如何解决 `Failed to initialize NVML: Unknown Error` 问题</h1>

    
    <div class="text-sm antialiased opacity-60">
      
      <time>Jun 14, 2024</time>
      
      
      
      
    </div>
    
  </header>

  <section><blockquote>
<p>本文是<a href="https://github.com/NVIDIA/nvidia-container-toolkit/issues/48">NOTICE: Containers losing access to GPUs with error: &ldquo;Failed to initialize NVML: Unknown Error&rdquo;</a>的中文翻译版本，内容有删减，亲测该方法有效。</p>
</blockquote>
<h1 id="如何解决-failed-to-initialize-nvml-unknown-error-问题">如何解决 <code>Failed to initialize NVML: Unknown Error</code> 问题</h1>
<h2 id="概述">概述</h2>
<p>在某些特定的情况下，我们发现k8s容器可能会突然从最初连接到的GPU上分离。我们已经确定了这个问题的根本原因，并确定了可能发生这种情况的受影响环境。在本文档的末尾提供了受影响环境的解决方法，直到发布适当的修复为止。</p>
<h2 id="问题总结">问题总结</h2>
<p>我们发现当使用container来管理GPU工作负载时，用户container可能会突然失去对GPU的访问权限。这种情况发生在使用systemd来管理容器的cgroups时，当触发重新加载任何包含对NVIDIA GPU的引用的Unit文件时（例如，通过执行<code>systemctl daemon-reload</code>）。</p>
<p>当你的container失去对GPU的访问权限时，你可能会看到类似于以下错误消息：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>Failed to initialize NVML: Unknown Error
</span></span></code></pre></div><p>一旦发生上述 ⬆️，就需要手动删除受影响的container，然后重新启动它们。</p>
<p>当container重新启动（手动或自动，取决于是否使用容器编排平台），它将重新获得对GPU的访问权限。</p>
<p>此问题的根源在于，最近版本的<code>runc</code>要求在<code>/dev/char</code>下面为注入到容器中的任何设备节点提供符号链接。不幸的是，NVIDIA设备并没有这些符号链接，NVIDIA GPU驱动也没有（当前）提供自动创建这些链接的方法</p>
<h2 id="受影响的环境">受影响的环境</h2>
<p><strong>如果你使用<code>runc</code>并在高级容器运行时（CRI）启用<code>systemd cgroup</code>管理的环境，那么你可能会受到这个问题的影响</strong></p>
<p>如果你没有使用<code>systemd</code>来管理<code>cgroup</code>，那么它就不会受到这个问题的影响。
下面是可能会受影响的的环境的详尽列表:</p>
<h3 id="使用containerdrunc的docker环境">使用<code>containerd</code>/<code>runc</code>的Docker环境</h3>
<h4 id="特定条件">特定条件</h4>
<ul>
<li>启用了<code>systemd</code>的<code>cgroup</code>驱动程序（例如，在<code>/etc/docker/daemon.json</code>中设置了参数<code>&quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]</code>）</li>
<li>使用了更新的<code>Docker</code>版本，其中<code>systemd cgroup</code>管理被默认设置的（即，在Ubuntu 22.04上）。</li>
</ul>
<blockquote>
<p><code>Note</code>如果你要检查<code>Docker</code>是否使用<code>systemd cgroup</code>管理，运行以下命令（下面的输出表示启用了<code>systemd cgroup</code>驱动程序）</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span> $ docker info  
</span></span><span style="display:flex;"><span> ...  
</span></span><span style="display:flex;"><span> Cgroup Driver: systemd  
</span></span><span style="display:flex;"><span> Cgroup Version: <span style="color:#ae81ff">1</span>
</span></span></code></pre></div><h3 id="使用containerdrunc的kubernetes环境">使用<code>containerd</code>/<code>runc</code>的Kubernetes环境</h3>
<h4 id="特定条件-1">特定条件</h4>
<ul>
<li>在<code>containerd</code>配置文件（通常位于：<code>/etc/containerd/config.toml</code>）中设置<code>SystemdCgroup = true</code>，如下所示：</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.nvidia.options<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>BinaryName <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/usr/local/nvidia/toolkit/nvidia-container-runtime&#34;</span>
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>SystemdCgroup <span style="color:#f92672">=</span> true
</span></span></code></pre></div><blockquote>
<p>Note:如果你想要检查<code>containerd</code>是否使用<code>systemd cgroup</code>管理，运行以下命令（下面的输出表示启用了<code>systemd cgroup</code>驱动程序）</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ sudo crictl info  
</span></span><span style="display:flex;"><span>...  
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;runtimes&#34;</span>: <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;nvidia&#34;</span>: <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;runtimeType&#34;</span>: <span style="color:#e6db74">&#34;io.containerd.runc.v2&#34;</span>,
</span></span><span style="display:flex;"><span>        ...
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;options&#34;</span>: <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>          <span style="color:#e6db74">&#34;BinaryName&#34;</span>: <span style="color:#e6db74">&#34;/usr/local/nvidia/toolkit/nvidia-container-runtime&#34;</span>,
</span></span><span style="display:flex;"><span>          ...
</span></span><span style="display:flex;"><span>          <span style="color:#e6db74">&#34;ShimCgroup&#34;</span>: <span style="color:#e6db74">&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#e6db74">&#34;SystemdCgroup&#34;</span>: true
</span></span></code></pre></div><h3 id="使用cri-orunc的kubernetes环境">使用<code>cri-o</code>/<code>runc</code>的Kubernetes环境</h3>
<h4 id="特定条件-2">特定条件</h4>
<p>在<code>cri-o</code>配置文件中启用了<code>systemd</code>的<code>cgroup_manager</code>（通常位于：<code>/etc/crio/crio.conf或/etc/crio/crio.conf.d/00-default</code>），如下所示（使用<code>OpenShift</code>的示例）：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#f92672">[</span>crio.runtime<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>cgroup_manager <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;systemd&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>hooks_dir <span style="color:#f92672">=</span> <span style="color:#f92672">[</span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;/etc/containers/oci/hooks.d&#34;</span>,
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;/run/containers/oci/hooks.d&#34;</span>,
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;/usr/share/containers/oci/hooks.d&#34;</span>,
</span></span><span style="display:flex;"><span><span style="color:#f92672">]</span>
</span></span></code></pre></div><p>Note: <code>Podman</code>环境默认使用<code>crun</code>，并且除非刻意将<code>runc</code>配置为要使用的底层容器运行CRI时，否则不会受到这个问题的影响。</p>
<h2 id="如何检查你的环境是否受到影响">如何检查你的环境是否受到影响</h2>
<p>您可以按照以下步骤确认您的系统是否受到影响。您可以这些步骤来确认错误是否可以复现。</p>
<h3 id="docker环境">Docker环境</h3>
<p>运行一个测试的<code>container</code>：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ docker run -d --rm --runtime<span style="color:#f92672">=</span>nvidia --gpus all <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --device<span style="color:#f92672">=</span>/dev/nvidia-uvm <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --device<span style="color:#f92672">=</span>/dev/nvidia-uvm-tools <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --device<span style="color:#f92672">=</span>/dev/nvidia-modeset <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --device<span style="color:#f92672">=</span>/dev/nvidiactl <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --device<span style="color:#f92672">=</span>/dev/nvidia0 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    nvcr.io/nvidia/cuda:12.0.0-base-ubuntu20.04 bash -c <span style="color:#e6db74">&#34;while [ true ]; do nvidia-smi -L; sleep 5; done&#34;</span>  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>bc045274b44bdf6ec2e4cc10d2968d1d2a046c47cad0a1d2088dc0a430add24b
</span></span></code></pre></div><blockquote>
<p>Note 请确保你测试的<code>container</code>挂载不同的设备（如上所示）。这是确定此特定问题的必要条件。</p>
</blockquote>
<p>如果您的系统有超过1个GPU，请在上述命令后附加额外的 &ndash;device 挂载。拥有2个GPU的系统的例子：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ docker run -d --rm --runtime<span style="color:#f92672">=</span>nvidia --gpus all <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    ...
</span></span><span style="display:flex;"><span>    --device<span style="color:#f92672">=</span>/dev/nvidia0 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --device<span style="color:#f92672">=</span>/dev/nvidia1 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    ...
</span></span></code></pre></div><p>此时可以查看<code>container</code>的log</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ docker logs bc045274b44bdf6ec2e4cc10d2968d1d2a046c47cad0a1d2088dc0a430add24b
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>GPU 0: Tesla K80 <span style="color:#f92672">(</span>UUID: GPU-05ea3312-64dd-a4e7-bc72-46d2f6050147<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>GPU 0: Tesla K80 <span style="color:#f92672">(</span>UUID: GPU-05ea3312-64dd-a4e7-bc72-46d2f6050147<span style="color:#f92672">)</span>
</span></span></code></pre></div><p>然后使用下面的命令来触发 <code>daemon-reload</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ sudo systemctl daemon-reload
</span></span></code></pre></div><p>此时再次查看<code>container</code>的log, 我们可以复现这个问题</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ docker logs bc045274b44bdf6ec2e4cc10d2968d1d2a046c47cad0a1d2088dc0a430add24b
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>GPU 0: Tesla K80 <span style="color:#f92672">(</span>UUID: GPU-05ea3312-64dd-a4e7-bc72-46d2f6050147<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>GPU 0: Tesla K80 <span style="color:#f92672">(</span>UUID: GPU-05ea3312-64dd-a4e7-bc72-46d2f6050147<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>GPU 0: Tesla K80 <span style="color:#f92672">(</span>UUID: GPU-05ea3312-64dd-a4e7-bc72-46d2f6050147<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>GPU 0: Tesla K80 <span style="color:#f92672">(</span>UUID: GPU-05ea3312-64dd-a4e7-bc72-46d2f6050147<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>Failed to initialize NVML: Unknown Error
</span></span><span style="display:flex;"><span>Failed to initialize NVML: Unknown Error
</span></span></code></pre></div><h3 id="k8s-环境">K8s 环境</h3>
<p>运行一个测试的<code>Pod</code>：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ cat nvidia-smi-loop.yaml
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>apiVersion: v1
</span></span><span style="display:flex;"><span>kind: Pod
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: cuda-nvidia-smi-loop
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  restartPolicy: OnFailure
</span></span><span style="display:flex;"><span>  containers:
</span></span><span style="display:flex;"><span>  - name: cuda
</span></span><span style="display:flex;"><span>    image: <span style="color:#e6db74">&#34;nvcr.io/nvidia/cuda:12.0.0-base-ubuntu20.04&#34;</span>
</span></span><span style="display:flex;"><span>    command: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;/bin/sh&#34;</span>, <span style="color:#e6db74">&#34;-c&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>    args: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;while true; do nvidia-smi -L; sleep 5; done&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>    resources:
</span></span><span style="display:flex;"><span>      limits:
</span></span><span style="display:flex;"><span>        nvidia.com/gpu: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ kubectl apply -f nvidia-smi-loop.yaml  
</span></span><span style="display:flex;"><span> 
</span></span><span style="display:flex;"><span>pod/cuda-nvidia-smi-loop created
</span></span></code></pre></div><p>此时可以查看<code>Pod</code>的log</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl logs cuda-nvidia-smi-loop
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>GPU 0: NVIDIA A100-PCIE-40GB <span style="color:#f92672">(</span>UUID: GPU-551720f0-caf0-22b7-f525-2a51a6ab478d<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>GPU 0: NVIDIA A100-PCIE-40GB <span style="color:#f92672">(</span>UUID: GPU-551720f0-caf0-22b7-f525-2a51a6ab478d<span style="color:#f92672">)</span>
</span></span></code></pre></div><p>然后使用下面的命令来触发 <code>daemon-reload</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ sudo systemctl daemon-reload
</span></span></code></pre></div><p>此时再次查看<code>Pod</code>的log, 我们可以复现这个问题</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl logs cuda-nvidia-smi-loop
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>GPU 0: NVIDIA A100-PCIE-40GB <span style="color:#f92672">(</span>UUID: GPU-551720f0-caf0-22b7-f525-2a51a6ab478d<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>GPU 0: NVIDIA A100-PCIE-40GB <span style="color:#f92672">(</span>UUID: GPU-551720f0-caf0-22b7-f525-2a51a6ab478d<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>Failed to initialize NVML: Unknown Error
</span></span><span style="display:flex;"><span>Failed to initialize NVML: Unknown Error
</span></span></code></pre></div><h2 id="workarounds">Workarounds</h2>
<p>下面的解决方法适用于独立的docker环境和k8s环境（按照推荐顺序👍提供多个选项；最推荐👍的选项在最上面）：</p>
<h3 id="docker环境-1">Docker环境</h3>
<h4 id="使用nvidia-ctk工具">使用<code>nvidia-ctk</code>工具:</h4>
<p>NVIDIA Container Toolkit v1.12.0包含了一个在<code>/dev/char</code>中为所有可能的NVIDIA设备节点创建符号链接的工具，这些节点对于在容器中使用GPU是必要的。该工具可以如下运行：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>sudo nvidia-ctk system create-dev-char-symlinks <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --create-all
</span></span></code></pre></div><p>这个命令应该在每个节点上的启动时运行，这样容器中使用GPU时就不会出现问题。在运行这个命令之前，需要确保NVIDIA驱动的内核模块已经加载。</p>
<p>一个简单的<code>udev</code>规则可以强制执行这一点，如下所示：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># This will create /dev/char symlinks to all device nodes</span>
</span></span><span style="display:flex;"><span>ACTION<span style="color:#f92672">==</span><span style="color:#e6db74">&#34;add&#34;</span>, DEVPATH<span style="color:#f92672">==</span><span style="color:#e6db74">&#34;/bus/pci/drivers/nvidia&#34;</span>, RUN<span style="color:#f92672">+=</span><span style="color:#e6db74">&#34;/usr/bin/nvidia-ctk system 	create-dev-char-symlinks --create-all&#34;</span>
</span></span></code></pre></div><p>安装此规则的推荐的地方是：<code>/lib/udev/rules.d/71-nvidia-dev-char.rules</code></p>
<p>在使用<code>NVIDIA GPU Driver Container</code>的情况下，必须指定驱动程序安装的路径。在这种情况下，命令应该修改为：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>sudo nvidia-ctk system create-dev-symlinks <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>        --create-all <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>        --driver-root<span style="color:#f92672">={{</span>NVIDIA_DRIVER_ROOT<span style="color:#f92672">}}</span>
</span></span></code></pre></div><p><code>{{NVIDIA_DRIVER_ROOT}}</code>是NVIDIA GPU Driver container安装NVIDIA GPU驱动程序并创建NVIDIA设备节点的路径。</p>
<h4 id="隐式地禁用docker中的systemd-cgroup管理">隐式地禁用Docker中的<code>systemd cgroup</code>管理</h4>
<p>设置<code>/etc/docker/daemon.json</code>文件中的参数<code>&quot;exec-opts&quot;: [&quot;native.cgroupdriver=cgroupfs&quot;]</code>并重启docker。</p>
<h4 id="降级dockerio">降级docker.io</h4>
<p>降级到<code>docker.io</code>包，其中<code>systemd</code>不是默认的<code>cgroup</code>管理器（当然不要覆盖）。</p>
<h3 id="k8s-环境-1">K8s 环境</h3>
<h4 id="gpu-operator">GPU Operator</h4>
<p>你可以使用<code>GPU Operator</code>的<code>22.9.2</code>以及以上版本，它会自动修复集群中所有K8s节点上的问题（此修复已经集成到<a href="https://github.com/NVIDIA/gpu-operator/tree/master/validator">gpu-operator-validator</a>中，当新Node部署或每次Node重启时都会运行）。</p>
<h4 id="k8s-device-plugin">k8s-device-plugin</h4>
<p>对于直接使用k8s-device-plugin的场景来说（即不通过<code>GPU Operator</code>使用），你可以安装如上一节所述的<code>udev</code>规则来解决此问题。。如果你使用了container来安装driver的话，请请确保传递正确的<code>{{NVIDIA_DRIVER_ROOT}}</code>。详情请参考<a href="https://github.com/NVIDIA/k8s-device-plugin?tab=readme-ov-file#configuration-option-details">configuration-option-details in k8s-device-plugin</a></p>
<h4 id="在containerd或cri-o隐式地禁用systemd-cgroup">在<code>containerd</code>或<code>cri-o</code>隐式地禁用<code>systemd cgroup</code></h4>
<p>从cri-o配置文件中删除参数<code>cgroup_manager = &quot;systemd&quot;</code>（通常位于：<code>/etc/crio/crio.conf</code>或<code>/etc/crio/crio.conf.d/00-default</code>），然后重启cri-o。</p>
<h4 id="降级containerd">降级<code>containerd</code></h4>
<p>降级到<code>containerd.io</code>包的一个版本，其中<code>systemd</code>不是默认的<code>cgroup</code>管理器（当然不要覆盖）。</p>
<h2 id="相关问题链接">相关问题链接🔗</h2>
<ul>
<li><a href="https://github.com/NVIDIA/nvidia-docker/issues/1671">&ldquo;Failed to initialize NVML: Unknown Error&rdquo; after random amount of time</a></li>
<li><a href="https://github.com/NVIDIA/nvidia-docker/issues/1730">NOTICE: Containers losing access to GPUs with error: &ldquo;Failed to initialize NVML: Unknown Error&rdquo; </a></li>
<li><a href="https://github.com/NVIDIA/nvidia-docker/issues/966#issuecomment-610928514">Updating cpu-manager-policy=static causes NVML unknown error</a></li>
</ul>
</section>

  
  

  
  
  
  
  

  
  

  
  

  


  
</article>


    </main>

    <footer
  class="opaco mx-auto flex h-[4.5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"
>
  <div class="mr-auto">
    &copy; 2024
    <a class="link" href="http://localhost:1313/">Tim Wang的技术博客</a>
  </div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >Powered by Hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >✎ Paper</a
  >
</footer>

  </body>
</html>
